<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Luca Di Liello's personal page</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.13.0/js/all.js" crossorigin="anonymous"></script>
        <link rel="stylesheet" href="css/academicons.min.css"/>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
        <!-- iconify source -->
        <script src="https://code.iconify.design/1/1.0.6/iconify.min.js"></script>
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top"
                ><span class="d-block d-lg-none">Luca Di Liello</span><span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="" /></span></a
            ><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#experience">Experience</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#education">Education</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#skills">Skills</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publications">Publications</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#software">Software</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#interests">Interests</a></li>
                </ul>
            </div>
        </nav>

        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">Luca <span class="text-primary">Di Liello</span></h1>
                    <div class="subheading mb-5">Via Sommarive 9 · Trento, TN 38123 · <a href="mailto:name@email.com">luca.diliello@unitn.it</a></div>
                    <p class="lead mb-5">I'm a PhD student in Information and Communication Technology at the University of Trento, Italy, where I also received my MS in Computer Science. 
                        I work mainly on Large Language Models (LLMs) for Natural Language Processing (NLP) and Information Retrieval (IR), but I am also interested in Computer Vision (CV) and constrained generative models.</p>
                    <p><b>Interests:</b> Natural Language Processing, Pre-Training of Neural Language Models, Question Answering (Answer Sentence Selection) and Generative models.</p>
                    <div class="social-icons">
                        <a href="https://www.semanticscholar.org/author/Luca-Di-Liello/1576607908" style="padding: 25px" >
                            <i class="ai ai-semantic-scholar-square ai-4x" aria-hidden="true"></i>
                        </a>

                        <a href="https://www.linkedin.com/in/luca-di-liello-4b624092/" style="padding: 25px" >
                            <i class="fab fa-linkedin fa-4x" aria-hidden="true"></i>
                        </a>
                        
                        <a href="https://github.com/lucadiliello" style="padding: 25px">
                            <i class="fab fa-github fa-4x" aria-hidden="true"></i>
                        </a>
                    </div>
                </div>
            </section>
            <hr class="m-0" />

            <!-- Experience-->
            <section class="resume-section" id="experience">
                <div class="resume-section-content">
                    <h2 class="mb-5">Experience</h2>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Applied Scientist Intern II</h3>
                            <div class="subheading mb-3">Amazon Alexa</div>
                            <p>Continuing the work of the previous internship about the design, development and test of large language models for Answer Sentence Selection.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">February 2022 - July 2022</span></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Applied Scientist Intern I</h3>
                            <div class="subheading mb-3">Amazon Alexa</div>
                            <p>Improve the state-of-the-art for Answer Sentence Selection models.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">April 2021 - October 2021</span></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Professor of Informatics</h3>
                            <div class="subheading mb-3">IISS Galileo Galilei Bolzano</div>
                            <p>Teach students the basics of Assembly, C and C++ languages, then give them the necessary notions to implement fast &amp; efficient algorithms.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">November 2018 - January 2019</span></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Big Data Analyst Intern</h3>
                            <div class="subheading mb-3">SpazioDati SRL</div>
                            <p>Analyze million of tweets, geolocated in Italy, to find interesting relations with companies and their empolyees.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">March 2017 - June 2017</span></div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />

            <!-- Education-->
            <section class="resume-section" id="education">
                <div class="resume-section-content">
                    <h2 class="mb-5">Education & Research</h2>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">University of Trento</h3>
                            <div class="subheading mb-3">PhD Candidate in Information and Communication Technologies</div>
                            <div><b>Topics</b> Natural Language Processing and Information Retrieval</div>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">November 2019 - Now</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">University of Trento</h3>
                            <div class="subheading mb-3">Master in Data Science</div>
                            <div><b>Topics</b> Big Data, Machine Learning</div>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2017 - October 2019</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">University of Trento</h3>
                            <div class="subheading mb-3">Bachelor in Computer Science</div>
                            <div><b>Topics</b> Computer Science, Algorithms, Databases Management, Web development</div>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2014 - July 2017</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">IISS Galileo Galilei Bolzano</h3>
                            <div class="subheading mb-3">Expert in Electronics and Telecommunications</div>
                            <div><b>Topics</b> Microelectronics, Telecommunications, Robotics, Low Level Programming</div>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2009 - June 2014</span></div>
                    </div>
                    
                </div>
            </section>
            <hr class="m-0" />

            <!-- Skills-->
            <section class="resume-section" id="skills">
                <div class="resume-section-content">
                    <h2 class="mb-5">Skills</h2>
                    <div class="subheading mb-3">Programming Languages</div>
                    <ul class="list-inline dev-icons">
                        <li class="list-inline-item">
                            <span class="iconify" data-icon="cib:c" data-inline="true"></span>
                        </li>
                        <li class="list-inline-item">
                            <i class="fab fa-html5"></i>
                        </li>
                        <li class="list-inline-item">
                            <i class="fab fa-css3-alt"></i>
                        </li>
                        <li class="list-inline-item">
                            <i class="fab fa-node-js"></i>
                        </li>
                        <li class="list-inline-item">
                            <i class="fab fa-java"></i>
                        </li>
                        <li class="list-inline-item">
                            <i class="fab fa-js"></i>
                        </li>
                        <li class="list-inline-item">
                            <i class="fab fa-npm"></i>
                        </li>
                        <li class="list-inline-item">
                            <i class="fab fa-python"></i>
                        </li>
                    </ul>

                    <div class="subheading mb-3">Tools</div>
                    <ul class="list-inline dev-icons">
                        <li class="list-inline-item">
                            <i class="fab fa-react"></i>
                        </li>
                        <li class="list-inline-item">
                            <i class="fab fa-wordpress"></i>
                        </li>
                        <li class="list-inline-item">
                            <span class="iconify" data-icon="cib:tensorflow" data-inline="true"></span>
                        </li>
                        <li class="list-inline-item">
                            <span class="iconify" data-icon="cib:pytorch" data-inline="true"></span>
                        </li>
                        <li class="list-inline-item">
                            <span class="iconify" data-icon="cib:flutter" data-inline="true"></span>
                        </li>
                        <li class="list-inline-item">
                            <span class="iconify" data-icon="cib:firebase" data-inline="true"></span>
                        </li>
                        <li class="list-inline-item">
                            <span class="iconify" data-icon="bi:lightning" data-inline="true"></span></span>
                        </li>
                    </ul>
                    <div class="subheading mb-3">Workflow</div>
                    <ul class="fa-ul mb-0">
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>Deep Learning models for NLP
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>Generative Models for Computer Vision
                        </li>
                    </ul>
                </div>
            </section>
            <hr class="m-0" />
            
            <!-- Publications-->
            <section class="resume-section" id="publications">
                <div class="resume-section-content">
                    <h2 class="mb-5">Publications</h2>
                    <ul>

                        <li> 
                            <h4>Context-aware Transformer Pre-Training for Answer Sentence Selection - 2023</h2>
                            <p>
                                Answer Sentence Selection (AS2) is a core component for building an accurate Question Answering pipeline. AS2 models rank a set of candidate sentences based on how likely they answer a given question. The state of the art in AS2 exploits pre-trained transformers by transferring them on large annotated datasets, while using local contextual information around the candidate sentence. In this paper, we propose three pre-training objectives designed to mimic the downstream fine-tuning task of contextual AS2. This allows for specializing LMs when fine-tuning for contextual AS2. Our experiments on three public and two large-scale industrial datasets show that our pre-training approaches (applied to RoBERTa and ELECTRA) can improve baseline contextual AS2 accuracy by up to 8% on some datasets.
                            </p>
                            <a href="https://assets.amazon.science/05/0f/8bcfcdbd4784864bd131d73b6719/context-aware-transformer-pre-training-for-answer-sentence-selection.pdf">PDF</a>
                            <p></p>
                        </li>

                        <li> 
                            <h4>Pre-training Transformer Models with Sentence-Level Objectives for Answer Sentence Selection - 2022</h2>
                            <p>
                                An important task for designing QA systems is answer sentence selection (AS2): selecting the sentence containing (or constituting) the answer to a question from a set of retrieved relevant documents. In this paper, we propose three novel sentence-level transformer pre-training objectives that incorporate paragraph-level semantics within and across documents, to improve the performance of transformers for AS2, and mitigate the requirement of large labeled datasets. Our experiments on three public and one industrial AS2 datasets demonstrate the empirical superiority of our pre-trained transformers over baseline models such as RoBERTa and ELECTRA for AS2.
                            </p>
                            <a href="https://aclanthology.org/2022.emnlp-main.810.pdf">PDF</a>
                            <p></p>
                        </li>

                        <li> 
                            <h4>Effective Pre-Training Objectives for Transformer-based Autoencoders - 2022</h2>
                            <p>
                                In this paper, we study trade-offs between efficiency, cost and accuracy when pre-training Transformer encoders with different pre-training objectives. For this purpose, we analyze features of common objectives and combine them to create new effective pre-training approaches. Specifically, we designed light token generators based on a straightforward statistical approach, which can replace ELECTRA computationally heavy generators, thus highly reducing cost. Our experiments also show that (i) there are more efficient alternatives to BERT's MLM, and (ii) it is possible to efficiently pre-train Transformer-based models using lighter generators without a significant drop in performance.
                            </p>
                            <a href="https://aclanthology.org/2022.findings-emnlp.405.pdf">PDF</a>
                            <p></p>
                        </li>

                        <li> 
                            <h4>TorchMetrics - Measuring Reproducibility in PyTorch - 2022</h2>
                            <p>
                                A main problem with reproducing machine learning publications is the variance of metric implementations across papers. A lack of standardization leads to different behavior in mechanisms such as checkpointing, learning rate schedulers or early stopping, that will influence the reported results. For example, a complex metric such as Fréchet inception distance (FID) for synthetic image quality evaluation (Heusel et al., 2017) will differ based on the specific interpolation method used.
                            </p>
                            <a href="https://www.theoj.org/joss-papers/joss.04101/10.21105.joss.04101.pdf">PDF</a>
                            <p></p>
                        </li>

                        <li> 
                            <h4>Paragraph-based Transformer Pre-training for Multi-Sentence Inference - 2022</h2>
                            <p>
                                Inference tasks such as answer sentence selection (AS2) or fact veriﬁcation are typically solved by ﬁne-tuning transformer-based models as individual sentence-pair classiﬁers. Recent studies show that these tasks beneﬁt from modeling dependencies across multiple candidate sentences jointly. In this paper, we ﬁrst show that popular pre-trained transformers perform poorly when used for ﬁne-tuning on multi-candidate inference tasks. We then propose a new pre-training objective that models the paragraph-level semantics across multiple input sentences. Our evaluation on three AS2 and one fact veriﬁcation datasets demonstrates the superiority of our pre-training technique over the traditional ones for transformers used as joint models for multi-candidate inference tasks, as well as when used as cross-encoders for sentence-pair formulations of these tasks.
                            </p>
                            <a href="https://aclanthology.org/2022.naacl-main.181.pdf">PDF</a>
                            <p></p>
                        </li>

                        <li> 
                            <h4>Efficient pre-training objectives for Transformers - 2021</h2>
                            <p>
                                Transformer-based neural networks have heavily impacted the field of natural language processing, outperforming most previous state-of-the-art models. However, well-known models such as BERT, RoBERTa, and GPT-2 require a huge compute budget to create a high quality contextualised representations. In this paper, we study several efficient pre-training objectives for Transformersbased models. By testing these objectives on different tasks, we determine which of the ELECTRA model’s new features is the most relevant: (i) Transformers pre-training can be improved when the input is not altered with artificial symbols, e.g., masked tokens; and (ii) loss functions computed using the whole output reduce training time. (iii) Additionally, we study efficient models composed of two blocks: a discriminator and a simple generator (inspired by the ELECTRA architecture). Our generator is based on a much simpler statistical approach, which minimally increases the computational cost. Our experiments show that it is possible to efficiently train BERT-like models using a discriminative approach as in ELECTRA but without a complex generator. Finally, we show that ELECTRA largely benefits from a deep hyper-parameter search.
                            </p>
                            <a href="https://arxiv.org/pdf/2104.09694.pdf">PDF</a>
                            <p></p>
                        </li>

                        <li> 
                            <h4>Language Transfer for Identifying Diagnostic Paragraphs in Clinical Notes - 2021</h2>
                            <p>
                                This paper aims at uncovering the structure of clinical documents, in particular, identifying paragraphs describing “diagnosis” or “procedures”. We present transformer-based architectures for approaching this task in a monolingual setting (English), exploring a weak supervision scheme. We further extend our contribution to a cross-lingual scenario, mitigating the need for expensive manual data annotation and taxonomy engineering for Italian.
                            </p>
                            <a href="http://ceur-ws.org/Vol-3033/paper56.pdf">PDF</a>
                            <p></p>
                        </li>

                        <li>
                            <h4>Cross-Language Transformer Adaptation for Frequently Asked Questions - 2020</h4>
                            <p>
                                Transfer learning has been proven to be effective, especially when data for the target domain/task is scarce. Sometimes data for a similar task is only available in another language because it may be very specific. In this paper, we explore the use of machine-translated data to transfer models on a related domain. Specifically, we transfer models from the question duplication task (QDT) to similar FAQ selection tasks. The source domain is the well-known English Quora dataset, while the target domain is a collection of small Italian datasets for real case scenarios consisting of FAQ groups retrieved by pivoting on common answers. Our results show great improvements in the zero-shot learning setting and modest improvements using the standard transfer approach for direct in-domain adaptation.
                            </p>
                            <a href="http://ceur-ws.org/Vol-2769/paper_85.pdf">PDF</a>
                            <p></p>
                        </li>

                        <li> 
                            <h4>Efficient Generation of Structured Objects with Constrained Adversarial Networks - 2020</h2>
                            <p>
                                Generative Adversarial Networks (GANs) struggle to generate structured objects like molecules and game maps. The issue is that structured objects must satisfy hard requirements (e.g., molecules must be chemically valid) that are difficult to acquire from examples alone. As a remedy, we propose Constrained Adversarial Networks (CANs), an extension of GANs in which the constraints are embedded into the model during training. This is achieved by penalizing the generator proportionally to the mass it allocates to invalid structures. In contrast to other generative models, CANs support efficient inference of valid structures (with high probability) and allows to turn on and off the learned constraints at inference time. CANs handle arbitrary logical constraints and leverage knowledge compilation techniques to efficiently evaluate the disagreement between the model and the constraints. Our setup is further extended to hybrid logical-neural constraints for capturing very complex constraints, like graph reachability. An extensive empirical analysis shows that CANs efficiently generate valid structures that are both high-quality and novel.
                            </p>
                            <a href="https://proceedings.neurips.cc/paper/2020/file/a87c11b9100c608b7f8e98cfa316ff7b-Paper.pdf">PDF</a>
                            <p></p>
                        </li>

                    <ul>
                </div>
            </section>

            <!-- Software-->
            <section class="resume-section" id="software">
                <div class="resume-section-content">
                    <h2 class="mb-5">Software</h2>
                    <ul>
                        <li>
                            <a href="https://github.com/lucadiliello/transformers-framework">Transformers Framework</a>
                        </li>
                        <li>
                            <a href="https://github.com/iKernels/transformers-lightning">Transformers Lightning</a>
                        </li>
                        <li>
                            <a href="https://github.com/lucadiliello/bleurt-pytorch">BLEURT-PyTorch</a>
                        </li>
                        <li>
                            <a href="https://github.com/lucadiliello/answer-selection">Answer Selection</a>
                        </li>
                        <li>
                            <a href="https://github.com/lucadiliello/mrqa-lightning">MRQA Lightning</a>
                        </li>
                        <li>
                            <a href="https://github.com/lucadiliello/semantic-loss-pytorch">Semantic Loss PyTorch</a>
                        </li>
                        <li>
                            <a href="https://github.com/lucadiliello/jsonboard">JsonBoard</a>
                        </li>
                    <ul>
                </div>
            </section>

            <!-- Interests-->
            <section class="resume-section" id="interests">
                <div class="resume-section-content">
                    <h2 class="mb-5">Interests</h2>
                    <p>
                        Apart from being a reseacher, I enjoy most of my time being outdoors or practicing my favourite sport: Kick Boxing.
                        During the summer, I try to play Volleyball as much as I can while in the winter I'm lucky to live close to many ski facilities.
                        When forced indoors, I love reading and watching tv-series like Rick &amp; Morty, Dr. House and Game of Thrones.
                    </p>
                </div>
            </section>
        </div>

        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
